#!/usr/bin/env python
import argparse
import os
import sys
import urllib
import pandas as pd
import subprocess
import glob
import nibabel as nib
import numpy as np
import sklearn
from nilearn.input_data import NiftiMasker
import nilearn
import csv

######################################
#Functions
######################################

def get_features_mtx(pheno_file, mask_img,input_dir,output_path):
    ''' Convert fMRI dreivatives to voxel wise features using only brain tissue voxels
    Args:
        pheno_file: A csv file with 2 columns, first containing subject IDs and second containing labels/dx
        mask_img: 3D nii file for separating brain from non-brain voxels 
        input_dir: path of input directory containing all image files
        output_path: path of output directory. Will write csv of missing data
    '''
# get the feature matrix X, true values Y, mask image
    missing_data=[]
    fnames=[]
    phenos = pd.read_csv(pheno_file,index_col=0)

    print('Parsing files')
    for ss in phenos.index:
        try:
            f = glob.glob(os.path.join(input_dir,'*%s*.nii.gz' % ss))[0]
            fnames.append(f)
        except:
            missing_data.append(ss)


    trimmed_phenos = phenos[~phenos.index.isin(missing_data)]
    Y=trimmed_phenos['DX_GROUP'].values # labels or diagnosis group or Y for classifier

    mask=nib.load(mask_img)
    masker=NiftiMasker(mask_img=mask)

    print('Generating feature matrix nsubjects x voxels')
    # only brain functional voxels
    X=masker.fit_transform(fnames)

    assert len(Y) == X.shape[0]
    
    print('Final feature matrix contains %d subjects and %d features' % (X.shape[0], X.shape[1]))
    
    np.savetxt(os.path.join(output_path,'missing_data.txt'),np.array([missing_data]), delimiter='\n',newline=os.linesep)
    
    return X,Y,masker

########################################
def trainModel(data, labels, classifier,modelDir,cv=True,k=5,masker=None,sparse= False,saveData=True):
    
    ''' Train a linear classifier and save the model weights to a csv file, nii image and pickle the model.
    Args:
        data (ndarray): A data matrix organized as nsamples x nfeatures 
        labels (ndarray): A 1d labels array same length as nsamples  
        classifer (str): type of algorithm to use; currently 'svm' and 'logistic' are implemented
        modelDir (str): directory to store csv, nii and pickle model files
        cv (bool; optional): whether to tune hyper-paramters using cross-validation; default True
        k (int; optional): how many CV folds to use during hyper-parameter tuning; default 5
        masker (sklearn obj; optional): the NIFTI masker used to map X used to reshape data for nii file; default
            masker generated by get_features_mtx
        sparse (bool; optional): whether to fit a sparse model using L1 regularization; default False
        saveData (bool; optional): whether to actually save csv, nii, and pickle model or just return model object; 
            default True
    '''

    from sklearn.metrics import accuracy_score
    
    if sparse:
        penalty = 'l1'
    else:
        penalty = 'l2'
                
    if classifier == 'svm':
        from sklearn.svm import LinearSVC
        algorithm = LinearSVC(penalty = penalty)
        if penalty == 'l1':
            algorithm.set_params(dual=False)
    elif classifier == 'logistic':
        from sklearn.linear_model import LogisticRegression
        algorithm = LogisticRegression(penalty = penalty)
        
    if cv:
        #CV and train model default 5-fold validation for hyperparameter tuning
        from sklearn.cross_validation import StratifiedKFold
        from sklearn.grid_search import GridSearchCV
        paramsToSearch = []        
        paramsToSearch.append({'C': [.001,.005,.01,.1,1,10]})
        clf = GridSearchCV(algorithm, paramsToSearch, cv=StratifiedKFold(labels, k))
    else:
        #Defaults and train model
        clf = algorithm
    
    clf.fit(data,labels)
    
    #Get training accuracy
    trainingPredictions = clf.predict(data)
    accuracy = accuracy_score(trainingPredictions, labels)
    print "Training accuracy: %f" % accuracy

    if cv:
        #Extract tuned model weights
        modelWeights = clf.best_estimator_.coef_[0]
        modelIntercept = clf.best_estimator_.intercept_
    else:
        #Extract deafult model weights
        modelWeights = clf.coef_[0]
        modelIntercept = clf.intercept_
    
    if saveData:
        print "Saving model weights, training accuracy and weights nii file"
        from sklearn.externals import joblib
        #Write intercept + model weights to csv and weight image to nii
        fileName = os.path.join(modelDir,classifier)
        np.savetxt(fileName + '_Weights.csv',np.concatenate([modelIntercept,modelWeights]), delimiter=',', newline=os.linesep)
        np.savetxt(fileName + '_TrainAcc.csv',np.array([accuracy]), delimiter=',',newline=os.linesep)
        masker.inverse_transform(modelWeights).to_filename(fileName + '_Weights.nii.gz')
        joblib.dump(clf, fileName + '_Model.pkl')
        
    return clf

######################################
def testModel(data, labels, classifier, modelDir, outputDir, saveData=True):
    
    ''' Test a linear classifier by loading a fitted model and returning predictions on given data.
    Args:
        data (ndarray): A data matrix organized as nsamples x nfeatures 
        labels (ndarray): A 1d labels array same length as nsamples  
        classifer (str): type of algorithm to use; currently 'svm' and 'logistic' are implemented
        modelDir (str): directory from which to load pickled model files
        outDir (str): directory to write csv file with testing accuracy and predictions
        saveData (bool; optional): whether to actually save csv or just return model object; 
            default True
    '''
    
    from sklearn.externals import joblib
    from sklearn.metrics import accuracy_score
        
    modelPath = os.path.join(modelDir,classifier)
    outPath = os.path.join(outputDir,classifier)
    #If model doesn't exist use csv with coefs - TODO
    clf = joblib.load(modelPath + '_Model.pkl')
    predictions = clf.predict(data)
    
    #Compute accuracy on test data
    accuracy = accuracy_score(predictions,labels)
    print "Testing accuracy: %f" % accuracy
    
    if saveData:
        from sklearn.externals import joblib
        #Save accuracy
        np.savetxt(outPath + '_TestAcc.csv', np.array([accuracy]), delimiter=',',newline=os.linesep)
        #Save predictions
        np.savetxt(outPath + '_Predictions.csv',predictions, delimiter=',',newline=os.linesep)
    
    return accuracy 


######################################
######################################

def run(command, env={}):
    process = Popen(command, stdout=PIPE, stderr=subprocess.STDOUT,
        shell=True, env=env)
    while True:
        line = process.stdout.readline()
        line = str(line, 'utf-8')[:-1]
        print(line)
        if line == '' and process.poll() != None:
            break


# Get arguments
parser = argparse.ArgumentParser(description='ABIDE Classifier Model')
parser.add_argument('--pheno_file', help='File containing the participant '
    'information for the analysis that will be run. This is a CSV with the '
    'participant id in the first column, the diagnosis in the'
    'second column.', required=True)
parser.add_argument('--input_dir', help='Directory with subject files', required=True)
parser.add_argument('--output_dir', help='Directory to contain output i.e. model weights and  model details',required=True)
parser.add_argument('--model_dir', help='Directory to store model objects if training and load model objects if testing',required=True)
parser.add_argument('--model', help='Type of classifier to use; one of: svm OR logistic', required=True)
parser.add_argument('--mask', help='Mask for non-zero brain tissue. Needs complete path',required=True)
parser.add_argument('--train', action='store_true')
parser.add_argument('--test', action='store_true')
parser.add_argument('--noCV',action='store_false',default=True, help='Indicate whether CV should be NOT used to tune hyperparameters and use sklearn defaults instead')
parser.add_argument('--k',default=5, help='Indicate number of CV folds to use during hyper-parameter tuning; defaults to 5 but ignored if --noCV is passed')
parser.add_argument('--sparse',action='store_true',default=False, help='Indicate whether a sparse model should be fit using L1 regularization; defaults to L2')
parser.add_argument('--nosave',action='store_false',default=True, help='Indicate whether models should be estimated but no files should be written; mostly for debugging')

# get the command line arguments
args = parser.parse_args()

# Check for correct and sufficient arguments
if args.train is None and args.test is None:
    parser.error('Enter flag for data: --train or --test')

# Check for mask
if not args.mask:
    parser.error('Need Mask Image')
else:
    mask_img=args.mask    

# Check for phenofile
if not args.pheno_file:
    parser.error('Need phenotype file with subject IDs and labels')
else:
    pheno_file = args.pheno_file

# Check for output directory
if not os.path.exists(args.output_dir):
    parser.error('Need output Directory!')
else:
    output_path=os.path.abspath(args.output_dir)
    
# Check for input directory
if not os.path.exists(args.input_dir):
    parser.error('Need input Directory!')
else:
    input_path=os.path.abspath(args.input_dir)
    
# Check for model directory
if not os.path.exists(args.model_dir):
    parser.error('Need model Directory!')
else:
    model_path=os.path.abspath(args.model_dir)
                                
if not args.model:
    parser.error('Must specify one of the following models: svm or logistic')
else:
    classifier=args.model

#Read input directory
input_dir = args.input_dir

# Get X or voxel wise features and Y for the classifier function in the sci-kit learn format
#subj_ids = np.genfromtxt(args.pheno_file, usecols=0, delimiter=',', skip_header=1, dtype='str')
X,Y,masker = get_features_mtx(pheno_file,mask_img,input_dir,output_path)


#########################################
if args.train:
    print('Training new %s' % args.model)
    # Train the model based on subject and feature matrix
    clf = trainModel(X,Y,classifier,model_path,cv=args.noCV,k=args.k,sparse=args.sparse,saveData=args.nosave,masker=masker)

elif args.test:
    # Test a previously trained model
    print('Testing previously trained %s model' % args.model)
    testModel(X,Y,classifier,model_path,output_path,saveData=args.nosave)
    

#########################################
